{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":118082,"databundleVersionId":14294892,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DA5401-2025-Data-Challenge","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U sentence-transformers transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\nlogin(secret_value)\n\nimport torch\nprint(torch.cuda.is_available())\nfrom sentence_transformers import SentenceTransformer\n\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom lightgbm import LGBMRegressor, early_stopping, log_evaluation\n\nfrom tqdm.notebook import tqdm\n\nfrom pathlib import Path\nDATA_DIR = Path(\"/kaggle/input/da5401-2025-data-challenge\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"def load_json(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = load_json(DATA_DIR / \"train_data.json\")\ntest  = load_json(DATA_DIR / \"test_data.json\")\nmetric_names = load_json(DATA_DIR / \"metric_names.json\")\nmetric_embs = np.load(DATA_DIR / \"metric_name_embeddings.npy\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(json.dumps(train[:2], indent = 2, ensure_ascii=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(json.dumps(test[:2], indent = 2, ensure_ascii=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json.dumps(metric_names, indent = 2, ensure_ascii=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(metric_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric_embs.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataframe(raw_json, metric_names, metric_name_embs, is_train=True):\n    \"\"\"\n    Creates a clean DataFrame for either train or test data.\n    \n    Parameters\n    ----------\n    raw_json : list[dict]\n        Parsed content of train_data.json or test_data.json.\n    metric_names : list[str]\n        Metric names corresponding to the metric embeddings.\n    metric_name_embs : np.ndarray\n        Precomputed embeddings for metric definitions.\n    is_train : bool, default=True\n        Whether this is training data (includes 'score' column).\n\n    Returns\n    -------\n    pd.DataFrame\n        Train: ['ID', 'metric_name', 'combined_text', 'metric_embedding', 'score']\n        Test : ['ID', 'metric_name', 'combined_text', 'metric_embedding']\n    \"\"\"\n\n    # Build name ‚Üí embedding index map\n    metric_to_idx = {name: i for i, name in enumerate(metric_names)}\n\n    rows = []\n    for i, item in enumerate(raw_json):\n        metric = item.get(\"metric_name\")\n        user = item.get(\"user_prompt\") or \"\"\n        response = item.get(\"response\") or \"\"\n        system = item.get(\"system_prompt\") or \"\"\n\n        # Concatenate into a single text field\n        combined_text = f\"User:{user}/Response:{response}/System:{system}\"\n\n        # Get embedding for the metric\n        emb = metric_name_embs[metric_to_idx[metric]].astype(np.float32) if metric in metric_to_idx else None\n\n        row = {\n            \"ID\": i + 1,\n            \"metric_name\": metric,\n            \"combined_text\": combined_text,\n            \"metric_name_embedding\": emb,\n        }\n\n        # Add score only if training data\n        if is_train:\n            row[\"score\"] = float(item.get(\"score\", 0.0))\n\n        rows.append(row)\n\n    df = pd.DataFrame(rows)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = create_dataframe(train, metric_names, metric_embs, is_train=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test  = create_dataframe(test, metric_names, metric_embs, is_train=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Embedding","metadata":{}},{"cell_type":"code","source":"model = SentenceTransformer(\"google/embeddinggemma-300m\", trust_remote_code=True)\n\ndef embed_texts_gemma(texts, batch_size=64):\n    # Gemma uses encode_document for text embeddings\n    return model.encode(\n        texts,\n        batch_size=batch_size,\n        show_progress_bar=True\n    ).astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pair_embeddings = embed_texts_gemma(df_train[\"combined_text\"].tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pair_embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pair_embeddings = embed_texts_gemma(df_test[\"combined_text\"].tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pair_embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(\"train_pair_embeddings.npy\", train_pair_embeddings)\nnp.save(\"test_pair_embeddings.npy\", test_pair_embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pair_embs = np.load(\"/kaggle/working/test_pair_embeddings.npy\")\ntest_pair_embs  = np.load(\"/kaggle/working/train_pair_embeddings.npy\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pair_embs.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pair_embs.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[\"context_embedding\"] = list(train_pair_embs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"context_embedding\"]  = list(test_pair_embs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.to_pickle(\"train_full.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.to_pickle(\"test_full.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_pickle(\"train_full.pkl\")\nprint(df_train.columns)\nprint(df_train.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['score'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_pickle(\"test_full.pkl\")\nprint(df_test.columns)\nprint(df_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combine features","metadata":{}},{"cell_type":"code","source":"def ensure_array(column, name=\"unknown\"):\n    \"\"\"\n    Convert a pandas column containing arrays/lists into a clean 2D NumPy array.\n    Handles irregular shapes, ensures float32 dtype, and logs issues once.\n    \"\"\"\n    try:\n        return np.vstack(column.values).astype(np.float32)\n    except Exception as e:\n        print(f\"Warning: irregular data in '{name}' ‚Äî using slower fallback. Error: {e}\")\n        arr = np.array([np.array(x, dtype=np.float32) for x in column.values])\n        return np.vstack(arr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_features(metric_embs, context_embs):\n    \"\"\"\n    Given metric and context embeddings, compute combined feature vectors.\n    Computes abs_diff and prod internally but returns only the essential\n    features (metric, context, cosine) for a compact representation.\n    \"\"\"\n    abs_diff = np.abs(metric_embs - context_embs)\n    prod = metric_embs * context_embs\n\n    denom = (\n        np.linalg.norm(metric_embs, axis=1, keepdims=True)\n        * np.linalg.norm(context_embs, axis=1, keepdims=True)\n        + 1e-8\n    )\n    cosine = np.sum(metric_embs * context_embs, axis=1, keepdims=True) / denom\n\n    features = np.concatenate([metric_embs, context_embs, cosine], axis=1)\n\n    # features = np.concatenate(\n    #     [metric_embs, context_embs, abs_diff, prod, cosine],\n    #     axis=1\n    # )\n\n    return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_embeddings(df, is_train=True):\n    \"\"\"\n    Clean and combine embeddings into a final numeric DataFrame.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain columns: \n        - 'metric_name_embedding'\n        - 'context_embedding'\n        - 'score' (if train)\n    is_train : bool, default=True\n        Whether the dataframe contains a target score column.\n\n    Returns\n    -------\n    pd.DataFrame\n        Fully numeric dataframe ready for modeling.\n        Includes 'score' column only if is_train=True.\n    \"\"\"\n    # Step 1: filter relevant columns\n    keep_cols = [\"metric_name_embedding\", \"context_embedding\"]\n    if is_train:\n        keep_cols.append(\"score\")\n    df_proc = df[keep_cols].copy()\n\n    # Step 2: convert embedding columns to numeric arrays\n    metric_embs = ensure_array(df_proc[\"metric_name_embedding\"], \"metric_name_embedding\")\n    context_embs = ensure_array(df_proc[\"context_embedding\"], \"context_embedding\")\n\n    # Step 3: compute interaction features\n    features = compute_features(metric_embs, context_embs)\n\n    # Step 4: create clean DataFrame\n    df_final = pd.DataFrame(features)\n    if is_train:\n        df_final[\"score\"] = df_proc[\"score\"].astype(float).values\n\n    return df_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_final = preprocess_embeddings(df_train, is_train=True)\ntest_final  = preprocess_embeddings(df_test, is_train=False)\n\nprint(\"train_final:\", train_final.shape)\nprint(\"test_final :\", test_final.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train_final.drop(columns=\"score\").values\ny = train_final[\"score\"].values\n\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training set:\", X_tr.shape, y_tr.shape)\nprint(\"Validation set:\", X_val.shape, y_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Function to submit","metadata":{}},{"cell_type":"code","source":"def build_and_save_submission(df_test, predictions, filename=\"submission.csv\", clip_range=(0, 10)):\n    \"\"\"\n    Build a submission DataFrame and save it to CSV.\n    \"\"\"\n    predictions = np.clip(predictions, clip_range[0], clip_range[1])\n    submission = pd.DataFrame({\n        \"ID\": df_test[\"ID\"].values,\n        \"score\": predictions\n    })\n    submission.to_csv(filename, index=False)\n    print(f\"Submission saved: {filename} ({len(submission)} rows)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## preprocessing","metadata":{}},{"cell_type":"code","source":"numeric_features = [col for col in train_final.columns if col != \"score\"]\n\npreprocessing_pipe = ColumnTransformer(\n    transformers=[\n        ('scaler', StandardScaler(), numeric_features),\n        ('pca', PCA(n_components=256, random_state=42), numeric_features)\n    ],\n    remainder=\"drop\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"### MLPRegressor","metadata":{}},{"cell_type":"code","source":"mlp = MLPRegressor(\n    hidden_layer_sizes=(128, 64),\n    activation=\"relu\",\n    solver=\"adam\",\n    learning_rate_init=0.001,\n    max_iter=500,\n    random_state=42\n)\n\nmlp_pipe = Pipeline([\n    ('preprocess', preprocessing_pipe),\n    ('model', mlp)\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlp_pipe.fit(X_tr, y_tr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_val_pred = mlp_pipe.predict(X_val)\nrmse = mean_squared_error(y_val, y_val_pred, squared=False)\nprint(f\"Validation RMSE: {rmse:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pred = mlp_pipe.predict(test_final.values)\n# submission = build_and_save_submission(df_test, test_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Prepare data\n# X = train_final.drop(columns=\"score\").values\n# y = train_final[\"score\"].values\n\n# # Custom RMSE scorer (since lower = better)\n# def rmse_scorer(est, Xv, yv):\n#     y_pred = est.predict(Xv)\n#     return -mean_squared_error(yv, y_pred, squared=False)  # negative for sklearn scoring\n\n# # Base pipeline\n# mlp_tune_pipe = Pipeline([\n#     (\"scaler\", StandardScaler()),\n#     (\"mlp\", MLPRegressor(\n#         max_iter=1000,          \n#         early_stopping=True,    \n#         n_iter_no_change=20,\n#         random_state=42\n#     ))\n# ])\n\n# param_dist = {\n#     \"mlp__hidden_layer_sizes\": [\n#         (256, 128),      # 2-layer balanced\n#         (512, 256, 128), # deeper but still reasonable\n#         (384, 192),      # mid-depth variant\n#         (256, 128, 64),  # tapering network\n#         (512, 256)       # high capacity\n#     ],\n\n#     # Nonlinearity ‚Äî relu usually wins, but tanh can sometimes generalize better\n#     \"mlp__activation\": [\"relu\", \"tanh\"],\n\n#     # Optimizer\n#     \"mlp__solver\": [\"adam\"],\n\n#     # Regularization ‚Äî embeddings respond well to light weight decay\n#     \"mlp__alpha\": np.logspace(-5, -1, 5),  # [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n\n#     # Learning rate ‚Äî too high will destroy cosine-sensitive embeddings\n#     \"mlp__learning_rate_init\": [1e-4, 3e-4, 1e-3, 3e-3],\n\n#     # Batch size ‚Äî balance between noise & stability\n#     \"mlp__batch_size\": [64, 128, 256],\n\n#     # Extra stability\n#     \"mlp__beta_1\": [0.8, 0.9, 0.95],  # momentum term for Adam\n#     \"mlp__beta_2\": [0.99, 0.999],     # adaptive decay term\n# }\n\n\n# # KFold config (5-fold is standard)\n# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# # Randomized search\n# search = RandomizedSearchCV(\n#     mlp_tune_pipe,\n#     param_distributions=param_dist,\n#     n_iter=50,                \n#     scoring=rmse_scorer,\n#     cv=kf,\n#     verbose=2,\n#     n_jobs=-1,\n#     random_state=42,\n# )\n\n# search.fit(X, y)\n\n# print(\"\\n=== Best MLP parameters ===\")\n# print(search.best_params_)\n# print(f\"Best CV RMSE: {-search.best_score_:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### lightgbm","metadata":{}},{"cell_type":"code","source":"# lgb_model = LGBMRegressor(\n#     objective=\"regression\",\n#     metric=\"rmse\",\n#     boosting_type=\"gbdt\",\n#     num_leaves=64,\n#     learning_rate=0.05,\n#     colsample_bytree=0.9,\n#     subsample=0.8,\n#     subsample_freq=5,\n#     min_child_samples=20,\n#     random_state=42,\n#     n_estimators=2000\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lgb_model.fit(\n#     X_tr, y_tr,\n#     eval_set=[(X_val, y_val)],\n#     eval_metric=\"rmse\",\n#     callbacks=[early_stopping(100), log_evaluation(100)]\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y_val_pred = lgb_model.predict(X_val)\n# rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n# print(f\"‚úÖ Validation RMSE: {rmse:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_pred = lgb_model.predict(test_final.values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LightGBM with kfold","metadata":{}},{"cell_type":"code","source":"# # ==========================================================\n# # Prepare train/test data\n# # ==========================================================\n# X = train_final.drop(columns=\"score\").values\n# y = train_final[\"score\"].values\n# X_test = test_final.values\n\n# # ==========================================================\n# # K-Fold configuration\n# # ==========================================================\n# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n# rmses = []\n# test_preds = np.zeros(len(X_test))\n\n# # ==========================================================\n# # Model parameters with moderate regularization\n# # ==========================================================\n# params = dict(\n#     objective=\"regression\",\n#     metric=\"rmse\",\n#     boosting_type=\"gbdt\",\n#     num_leaves=48,             # smaller trees = less overfit\n#     learning_rate=0.05,\n#     colsample_bytree=0.7,      # randomly select features per tree\n#     subsample=0.7,             # randomly sample rows\n#     subsample_freq=5,\n#     min_child_samples=50,      # require more samples per leaf\n#     reg_alpha=0.5,             # L1 regularization\n#     reg_lambda=0.8,            # L2 regularization\n#     min_gain_to_split=0.01,    # penalize small gains\n#     random_state=42,\n#     n_estimators=2000\n# )\n\n# # ==========================================================\n# # K-Fold cross-validation loop\n# # ==========================================================\n# for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), 1):\n#     print(f\"\\nüöÄ Fold {fold} training...\")\n#     X_tr, X_val = X[train_idx], X[val_idx]\n#     y_tr, y_val = y[train_idx], y[val_idx]\n\n#     model = LGBMRegressor(**params)\n\n#     model.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_val, y_val)],\n#         eval_metric=\"rmse\",\n#         callbacks=[early_stopping(100), log_evaluation(0)]\n#     )\n\n#     # Validation predictions\n#     y_val_pred = model.predict(X_val)\n#     rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n#     rmses.append(rmse)\n#     print(f\"‚úÖ Fold {fold} RMSE: {rmse:.4f}\")\n\n#     # Blend test predictions\n#     test_preds += model.predict(X_test) / kf.n_splits\n\n# # ==========================================================\n# # Cross-validation summary\n# # ==========================================================\n# print(\"\\nüéØ Cross-validation RMSEs:\", np.round(rmses, 4))\n# print(f\"üèÅ Mean CV RMSE: {np.mean(rmses):.4f} ¬± {np.std(rmses):.4f}\")\n\n# # ==========================================================\n# # Build and save submission\n# # ==========================================================\n# submission = build_and_save_submission(df_test, test_preds, filename=\"submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# submission = build_and_save_submission(df_test, test_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}